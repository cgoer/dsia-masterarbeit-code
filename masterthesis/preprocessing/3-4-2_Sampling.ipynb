{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ccaf2d4",
   "metadata": {},
   "source": [
    "# Sampling der Daten zur Annotation\n",
    "\n",
    "In diesem Notebook werden die Daten ausgew√§hlt, welche im sp√§teren Verlauf zur Annotation verwendet werden.\n",
    "Ebenfalls werden diese Daten zur Verwendung im Annotationstool aufbereitet.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5198e0c4",
   "metadata": {},
   "source": [
    "### Package- und Datenimport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592c4013",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df220a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_client():\n",
    "    return MongoClient('mongodb://{}:{}@{}:{}'.format(\n",
    "        'root',\n",
    "        'root',\n",
    "        '0.0.0.0',\n",
    "        '27017',\n",
    "    ))\n",
    "\n",
    "def get_database():\n",
    "    client = get_client()\n",
    "    return client.get_database('masterthesis-goerner')\n",
    "\n",
    "db = get_database()\n",
    "cursor = db['preprocessed'].find({})\n",
    "preprocessed =  pd.DataFrame(list(cursor))\n",
    "cursor_prsmpl = db['presampled'].find({})\n",
    "presampled = pd.DataFrame(list(cursor_prsmpl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ddf346",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed['date'] = pd.to_datetime(preprocessed['date_publish']).dt.date\n",
    "preprocessed = preprocessed.merge(presampled, on=\"_id\",suffixes=('', '_y'))\n",
    "preprocessed = preprocessed.drop(columns=['url_id_y', 'preprocessed_word_y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bb8b7e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Analyse der Sentiment-Verteilung\n",
    "\n",
    "In diesem Abschnitt werden die zuvor ermittelten Sentiment-Scores zu Klassen transformiert und die Verteilungen im Datensatz analysiert.\n",
    "Es erfolgt eine Anpassung der Verteilung von S√§tzen mit neutralem Sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d90de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_sen(polarity_score):\n",
    "    if polarity_score > 0.2:\n",
    "        return 'positive'\n",
    "    elif polarity_score < -0.2:\n",
    "        return 'negative'\n",
    "    return 'neutral'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131cd0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed['sentiment'] = preprocessed['polarity'].map(categorize_sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19f5c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed['sentiment'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9886b2a9",
   "metadata": {},
   "source": [
    "Die Klasse der neutralen Sentiments ist deutlich h√∂her als alle weiteren Klassen. Daher werden einige S√§tze mit neutralem Sentiment entfernt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa937fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "neutrals = preprocessed[preprocessed['sentiment'] == 'neutral']\n",
    "neutrals_smpl = neutrals.sample(frac=0.5, random_state=1)\n",
    "preprocessed = preprocessed[~preprocessed.index.isin(list(neutrals_smpl.index))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6481b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed['sentiment'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0e37ec",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### A1: Auswahl von Satz-Paaren mit demselben Sentiment-Target\n",
    "\n",
    "In diesem Schritt werden 1000 Satz-Paare gesampled, welche dieselbe Sentiment-Target am selben Ver√∂ffentlichungsdatum addressieren. Dabei werden lediglich Satz-Paare jeweils verschiedener Nachrichtenanbieter verwendet. Die gesampleten Daten werden aus dem Urspr√ºnglichen Datenpool entfernt, um Mehrfachauswahl zu vermeiden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8d8d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract all with match\n",
    "matches = preprocessed[preprocessed.matches.str.len() > 0]\n",
    "# drop all entries from same article to have more variance\n",
    "matches = matches.drop_duplicates(subset='url_id')\n",
    "# sample all extracts, take 1k samples to have 2k results later\n",
    "sample = matches.sample(frac=0.003645829536, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e092500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract matches from original df\n",
    "def extract_matches_from_df(df, row):\n",
    "    for match in row['matches']:\n",
    "        match_df = df[df['url_id'] == match]\n",
    "        match_df = match_df[match_df['preprocessed_word'] == row['preprocessed_word']]\n",
    "        if len(match_df) > 0:\n",
    "            mtch = match_df['_id'].values\n",
    "            return str(mtch[0])\n",
    "    return None\n",
    "sample['match_id'] = sample.apply(lambda x: extract_matches_from_df(preprocessed, x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd914327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add matches to sample df\n",
    "def extract_matches(df, match_id, original_id):\n",
    "    match = df[df['_id'].astype(str) == match_id]\n",
    "    match['match_id'] = str(original_id)\n",
    "    return match\n",
    "\n",
    "# extract match_ids from original dataframe and append them to sample\n",
    "match_counterparts = sample.apply(lambda x: extract_matches(preprocessed, x.match_id, x._id), axis=1)\n",
    "for match_coutnerpart in match_counterparts:\n",
    "    sample = pd.concat([sample, match_coutnerpart])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94538cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove samples from preprocessed\n",
    "preprocessed = preprocessed[~preprocessed['_id'].astype(str).isin(sample['_id'].astype(str))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646d6e80",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### A2: Auswahl von S√§tzen aller Targets eines Artikels\n",
    "\n",
    "In diesem Abschnitt werden 2000 S√§tze gesampled. Die verwendeten S√§tze entstammen jeweils einem Nachrichtenartikel, dabei sind maximal drei s√§tze je Artikel vorhanden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ac18aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove sentences where not all three sentences are present for an article\n",
    "less_than_3_occurences_per_article = preprocessed.url_id.value_counts().reset_index(name=\"count\").query(\"count < 3\")[\"index\"]\n",
    "less_than_3_occurences_per_article = list(less_than_3_occurences_per_article.values)\n",
    "cleaned = preprocessed[~preprocessed['url_id'].isin(less_than_3_occurences_per_article)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8355b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear duplicates in url_ids (to have only one of 3 per article) and sample 1/3 of 2000 Articles\n",
    "cleaned = cleaned.drop_duplicates(subset='url_id')\n",
    "cleaned_sampl = cleaned.sample(frac=0.003003178777, random_state=1)\n",
    "\n",
    "# retrieve all articles with the selected url ids from original\n",
    "res = preprocessed[preprocessed['url_id'].isin(list(cleaned_sampl.url_id))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e440d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add results to sample data, remove selected articles from data pool\n",
    "sample = pd.concat([sample, res])\n",
    "preprocessed = preprocessed[~preprocessed['_id'].astype(str).isin(sample['_id'].astype(str))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff77523",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### A3: Auswahl zuf√§lliger S√§tze\n",
    "\n",
    "In diesem Schritt werden weitere zuf√§llige S√§tze dem Sample hinzugef√ºgt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dead02",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_a3 = preprocessed.sample(frac=0.003464590396, random_state=1)\n",
    "sample = pd.concat([sample, preprocessed_a3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162cc1d7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Vorbereitung der Daten f√ºr den Upload bei Toloka\n",
    "\n",
    "Die Daten werden nun in ein Toloka-spezifisches Format transformiert. Dabei werden die Schl√ºsselw√∂rter im Text markiert und die Daten schlie√ülich als .tsv exportiert. Auch werden die Samples in der Datenbank abgelegt, um eine Text-Referenz zwischen Toloka und der lokal genutzten ID zu erhalten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88462075",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample['preprocessed_id'] = sample['_id']\n",
    "sample = sample.drop(columns=['_id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba01f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_marks(text, mentions):\n",
    "    for mention in mentions:\n",
    "        # Use ùüá sign instead of * directly, since articles sometimes contain * and this breaks .md layout then.\n",
    "        # *s also cannot be removed before, since then the start and end pos doesn't work anymore.. therefore use uncommon sign and replace it later\n",
    "        # also, using 'further mentions' here makes problems if the start and end pos overlaps, so only take main words\n",
    "        if mention['type'] == 'main':\n",
    "            text = text[:int(mention['start_pos'])] + 'ùüáùüá' + text[int(mention['start_pos']):int(mention['end_pos'])] + 'ùüáùüá' + text[int(mention['end_pos']):]\n",
    "    text = text.replace('\"', '\\\"')\n",
    "    text = text.replace('*', '')\n",
    "    text = text.replace('ùüá', '*')\n",
    "    return text\n",
    "sample['sentence_toloka'] = sample.apply(lambda x: add_marks(x.sentence, x.mentions), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab44e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop faulty dataset\n",
    "sample = sample.drop([5456])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb84f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to DB\n",
    "db.sample.insert_many(sample.to_dict('records'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb8f710",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = sample.drop(columns=['_id', 'sentiment', 'polarity', 'matches', 'date', 'tfidf_score'])\n",
    "sample['INPUT:sentence'] = sample['sentence_toloka']\n",
    "sample.to_csv('toloka.tsv', sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7360ef3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Vorbereitung der Daten f√ºr dn Upload bei Doccano\n",
    "\n",
    "Die Daten werden nun in ein Doccano-spezifisches Format transformiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8555712",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample['text'] = sample['sentence']\n",
    "sample['entities'] = sample.apply(lambda x: [[int(x.main_start_pos),int(x.main_end_pos),x.original_word_main]], axis=1)\n",
    "sample = sample.drop(columns=['_id', 'date_publish', 'source_domain', 'url_id','preprocessed_word','original_word_main','sentence','main_start_pos','main_end_pos','mentions','match_id','preprocessed_id','INPUT:sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7918f5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.head(250).to_json('doccano.jsonl',orient='records', lines=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
